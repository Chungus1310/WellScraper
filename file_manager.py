import os
import shutil
import json
from datetime import datetime, timedelta
import glob

class FileManager:
    """Utility class for managing extraction files and folders"""
    
    def __init__(self, base_directory="extractions"):
        self.base_directory = base_directory
        self.ensure_base_directory()
    
    def ensure_base_directory(self):
        """Ensure the base extraction directory exists"""
        os.makedirs(self.base_directory, exist_ok=True)
    
    def create_extraction_folder(self, keyword, location):
        """Create organized folder structure for extraction results"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        safe_keyword = self._sanitize_filename(keyword)
        safe_location = self._sanitize_filename(location)
        
        base_folder = f"{self.base_directory}/{safe_keyword}_{safe_location}_{timestamp}"
        
        # Create folder structure
        folders = {
            'base': base_folder,
            'results': f"{base_folder}/results",
            'debug': f"{base_folder}/debug",
            'temp': f"{base_folder}/temp",
            'screenshots': f"{base_folder}/debug/screenshots",
            'html_dumps': f"{base_folder}/debug/html_dumps"
        }
        
        for folder in folders.values():
            os.makedirs(folder, exist_ok=True)
        
        # Create README file
        self._create_readme(folders['base'], keyword, location)
        
        return folders
    
    def _sanitize_filename(self, filename):
        """Sanitize filename to be safe for filesystem"""
        # Remove or replace invalid characters
        invalid_chars = '<>:"/\\|?*'
        for char in invalid_chars:
            filename = filename.replace(char, '_')
        return filename.strip()
    
    def _create_readme(self, folder_path, keyword, location):
        """Create a README file for the extraction folder"""
        readme_content = f"""# Job Extraction Results

## Extraction Details
- **Keyword**: {keyword}
- **Location**: {location}
- **Extraction Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Generated by**: Wellfound Job Scraper

## Folder Structure
- `results/` - Contains the main extraction files (JSON)
- `debug/` - Contains debug information
  - `screenshots/` - Browser screenshots
  - `html_dumps/` - Raw HTML files
- `temp/` - Temporary files (cleaned up after extraction)

## Files Description
- `jobs_search_*.json` - Basic job listings from search results
- `jobs_detailed_*.json` - Detailed job information with full descriptions
- `summary_*.json` - Extraction summary and statistics

## Usage
The JSON files can be imported into any data analysis tool or used with the provided Python scripts for further processing.
"""
        
        with open(f"{folder_path}/README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)
    
    def move_debug_files(self, folders):
        """Move debug files to appropriate debug folders"""
        debug_files_map = {
            'debug_search_screenshot.png': 'screenshots/search_screenshot.png',
            'debug_search_page.html': 'html_dumps/search_page.html',
            'debug_job_screenshot.png': 'screenshots/job_screenshot.png',
            'debug_job_page.html': 'html_dumps/job_page.html'
        }
        
        moved_files = []
        for original_file, debug_path in debug_files_map.items():
            if os.path.exists(original_file):
                destination = f"{folders['debug']}/{debug_path}"
                os.makedirs(os.path.dirname(destination), exist_ok=True)
                shutil.move(original_file, destination)
                moved_files.append(destination)
        
        return moved_files
    
    def cleanup_temp_files(self, folders):
        """Clean up temporary files and folders"""
        try:
            # Remove temp folder if exists
            if os.path.exists(folders['temp']):
                shutil.rmtree(folders['temp'])
            
            # Clean up any remaining debug files in root
            debug_patterns = [
                'debug_*.png',
                'debug_*.html',
                '*.tmp',
                'chromedriver.log'
            ]
            
            cleaned_files = []
            for pattern in debug_patterns:
                for file in glob.glob(pattern):
                    try:
                        os.remove(file)
                        cleaned_files.append(file)
                    except:
                        pass
            
            return cleaned_files
            
        except Exception as e:
            print(f"Cleanup error: {str(e)}")
            return []
    
    def list_extractions(self):
        """List all available extractions with metadata"""
        extractions = []
        
        if not os.path.exists(self.base_directory):
            return extractions
        
        for item in os.listdir(self.base_directory):
            item_path = os.path.join(self.base_directory, item)
            if os.path.isdir(item_path):
                try:
                    # Parse extraction info from folder name
                    # Handle both old and new formats
                    
                    # Find the timestamp pattern from the end (YYYYMMDD_HHMMSS)
                    parts = item.split('_')
                    
                    # Look for timestamp pattern (8 digits + 6 digits at the end)
                    if (len(parts) >= 3 and 
                        len(parts[-2]) == 8 and parts[-2].isdigit() and 
                        len(parts[-1]) == 6 and parts[-1].isdigit()):
                        # Found timestamp at the end
                        timestamp = f"{parts[-2]}_{parts[-1]}"
                        remaining_parts = parts[:-2]
                        
                        if len(remaining_parts) >= 2:
                            # Last remaining part is location, others are keyword
                            location = remaining_parts[-1]
                            keyword_parts = remaining_parts[:-1]
                            keyword = ' '.join(keyword_parts)  # Join with spaces
                        elif len(remaining_parts) == 1:
                            keyword = remaining_parts[0]
                            location = 'unknown'
                        else:
                            keyword = 'unknown'
                            location = 'unknown'
                    else:
                        # No timestamp found, try to parse as best as possible
                        if len(parts) >= 2:
                            location = parts[-1]
                            keyword = ' '.join(parts[:-1])
                        elif len(parts) == 1:
                            keyword = parts[0]
                            location = 'unknown'
                        else:
                            keyword = 'unknown'
                            location = 'unknown'
                        timestamp = 'unknown'
                    
                    # Convert dashes back to spaces if present (for new format)
                    keyword = keyword.replace('-', ' ')
                    location = location.replace('-', ' ')
                        
                    # Get folder stats
                    results_path = os.path.join(item_path, 'results')
                    total_jobs = 0
                    completed_jobs = 0
                    
                    if os.path.exists(results_path):
                        # Try to read from summary file first
                        summary_files = [f for f in os.listdir(results_path) if f.startswith('summary_') and f.endswith('.json')]
                        if summary_files:
                            try:
                                with open(os.path.join(results_path, summary_files[0]), 'r', encoding='utf-8') as f:
                                    summary_data = json.load(f)
                                    if 'results_summary' in summary_data:
                                        rs = summary_data['results_summary']
                                        total_jobs = rs.get('total_jobs_found', rs.get('total_jobs_processed', 0))
                                        completed_jobs = rs.get('successful_extractions', 0)
                            except Exception as e:
                                print(f"Error reading summary file: {e}")
                        
                        # Fallback: read from detailed jobs file
                        if total_jobs == 0:
                            detailed_files = [f for f in os.listdir(results_path) if f.startswith('jobs_detailed_') and f.endswith('.json')]
                            if detailed_files:
                                try:
                                    with open(os.path.join(results_path, detailed_files[0]), 'r', encoding='utf-8') as f:
                                        data = json.load(f)
                                        if isinstance(data, dict):
                                            if 'jobs' in data:
                                                total_jobs = len(data['jobs'])
                                                completed_jobs = len([job for job in data['jobs'] if job])
                                            elif 'extraction_info' in data:
                                                ei = data['extraction_info']
                                                total_jobs = ei.get('total_jobs', 0)
                                                completed_jobs = ei.get('completed_jobs', 0)
                                        elif isinstance(data, list):
                                            total_jobs = len(data)
                                            completed_jobs = len([job for job in data if job])
                                except Exception as e:
                                    print(f"Error reading detailed jobs file: {e}")
                    
                    extractions.append({
                        'id': item,
                        'keyword': keyword,
                        'location': location,
                        'timestamp': timestamp,
                        'total_jobs': total_jobs,
                        'completed_jobs': completed_jobs,
                        'job_count': total_jobs,  # Keep for backward compatibility
                        'created_date': datetime.fromtimestamp(os.path.getctime(item_path)).isoformat(),
                        'size_mb': self._get_folder_size(item_path) / (1024 * 1024)
                    })
                except Exception as e:
                    print(f"Error parsing extraction folder {item}: {e}")
                    # If parsing fails, still include the folder
                    extractions.append({
                        'id': item,
                        'keyword': 'Unknown',
                        'location': 'Unknown',
                        'timestamp': 'Unknown',
                        'total_jobs': 0,
                        'completed_jobs': 0,
                        'job_count': 0,
                        'created_date': datetime.fromtimestamp(os.path.getctime(item_path)).isoformat(),
                        'size_mb': self._get_folder_size(item_path) / (1024 * 1024)
                    })
        
        # Sort by creation date (newest first)
        extractions.sort(key=lambda x: x['created_date'], reverse=True)
        return extractions

    def get_extraction_summary(self, extraction_id):
        """Get detailed summary of a specific extraction"""
        extraction_path = os.path.join(self.base_directory, extraction_id)
        
        if not os.path.exists(extraction_path):
            raise FileNotFoundError(f"Extraction {extraction_id} not found")
        
        summary = {
            'id': extraction_id,
            'path': extraction_path,
            'created_date': datetime.fromtimestamp(os.path.getctime(extraction_path)).isoformat(),
            'size_mb': self._get_folder_size(extraction_path) / (1024 * 1024),
            'files': {}
        }
        
        # Analyze each subfolder
        for subfolder in ['results', 'debug', 'temp']:
            subfolder_path = os.path.join(extraction_path, subfolder)
            if os.path.exists(subfolder_path):
                files = []
                for file in os.listdir(subfolder_path):
                    file_path = os.path.join(subfolder_path, file)
                    if os.path.isfile(file_path):
                        files.append({
                            'name': file,
                            'size_kb': os.path.getsize(file_path) / 1024,
                            'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat()
                        })
                summary['files'][subfolder] = files
        
        return summary

    def delete_extraction(self, extraction_id):
        """Delete an extraction and all its files"""
        extraction_path = os.path.join(self.base_directory, extraction_id)
        
        if not os.path.exists(extraction_path):
            raise FileNotFoundError(f"Extraction {extraction_id} not found")
        
        shutil.rmtree(extraction_path)
        return True

    def export_results(self, extraction_id, format='json'):
        """Export extraction results in the specified format"""
        extraction_path = os.path.join(self.base_directory, extraction_id)
        
        if not os.path.exists(extraction_path):
            raise FileNotFoundError(f"Extraction {extraction_id} not found")
        
        results_path = os.path.join(extraction_path, 'results')
        
        # Find the detailed jobs file
        detailed_file = None
        for file in os.listdir(results_path):
            if file.startswith('jobs_detailed_') and file.endswith('.json'):
                detailed_file = os.path.join(results_path, file)
                break
        
        if not detailed_file:
            raise FileNotFoundError("No detailed jobs file found")
        
        # Load the data
        with open(detailed_file, 'r', encoding='utf-8') as f:
            jobs_data = json.load(f)
        
        if format == 'json':
            export_path = os.path.join(extraction_path, f'export_{extraction_id}.json')
            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(jobs_data, f, ensure_ascii=False, indent=2)
            return export_path
        
        elif format == 'csv':
            import csv
            export_path = os.path.join(extraction_path, f'export_{extraction_id}.csv')
            
            if jobs_data:
                # Flatten the job data for CSV
                flattened_data = []
                for job in jobs_data:
                    flat_job = {}
                    for key, value in job.items():
                        if isinstance(value, (dict, list)):
                            flat_job[key] = json.dumps(value)
                        else:
                            flat_job[key] = value
                    flattened_data.append(flat_job)
                
                # Write CSV
                with open(export_path, 'w', newline='', encoding='utf-8') as f:
                    if flattened_data:
                        writer = csv.DictWriter(f, fieldnames=flattened_data[0].keys())
                        writer.writeheader()
                        writer.writerows(flattened_data)
            
            return export_path
        
        else:
            raise ValueError(f"Unsupported format: {format}")

    def cleanup_old_extractions(self, days=7):
        """Clean up extractions older than specified days"""
        cutoff_date = datetime.now() - timedelta(days=days)
        deleted_count = 0
        
        if not os.path.exists(self.base_directory):
            return deleted_count
        
        for item in os.listdir(self.base_directory):
            item_path = os.path.join(self.base_directory, item)
            if os.path.isdir(item_path):
                # Check if folder is older than cutoff
                creation_date = datetime.fromtimestamp(os.path.getctime(item_path))
                if creation_date < cutoff_date:
                    shutil.rmtree(item_path)
                    deleted_count += 1
        
        return deleted_count
    
    def move_debug_files(self, folders):
        """Move debug files to appropriate debug folders"""
        debug_files_map = {
            'debug_search_screenshot.png': 'screenshots/search_screenshot.png',
            'debug_search_page.html': 'html_dumps/search_page.html',
            'debug_job_screenshot.png': 'screenshots/job_screenshot.png',
            'debug_job_page.html': 'html_dumps/job_page.html'
        }
        
        moved_files = []
        for original_file, debug_path in debug_files_map.items():
            if os.path.exists(original_file):
                destination = f"{folders['debug']}/{debug_path}"
                os.makedirs(os.path.dirname(destination), exist_ok=True)
                shutil.move(original_file, destination)
                moved_files.append(destination)
        
        return moved_files
    
    def cleanup_temp_files(self, folders):
        """Clean up temporary files and folders"""
        try:
            # Remove temp folder if exists
            if os.path.exists(folders['temp']):
                shutil.rmtree(folders['temp'])
            
            # Clean up any remaining debug files in root
            debug_patterns = [
                'debug_*.png',
                'debug_*.html',
                '*.tmp',
                'chromedriver.log'
            ]
            
            cleaned_files = []
            for pattern in debug_patterns:
                for file in glob.glob(pattern):
                    try:
                        os.remove(file)
                        cleaned_files.append(file)
                    except:
                        pass
            
            return cleaned_files
            
        except Exception as e:
            print(f"Cleanup error: {str(e)}")
            return []
    
    def get_extraction_summary(self, folder_path):
        """Get summary of extraction folder contents"""
        if not os.path.exists(folder_path):
            return None
        
        summary = {
            'folder_path': folder_path,
            'creation_time': datetime.fromtimestamp(os.path.getctime(folder_path)),
            'size_mb': self._get_folder_size(folder_path) / (1024 * 1024),
            'files': {
                'results': [],
                'debug': [],
                'other': []
            }
        }
        
        # Scan files
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, folder_path)
                file_info = {
                    'name': file,
                    'path': relative_path,
                    'size_kb': os.path.getsize(file_path) / 1024,
                    'modified': datetime.fromtimestamp(os.path.getmtime(file_path))
                }
                
                if 'results' in relative_path:
                    summary['files']['results'].append(file_info)
                elif 'debug' in relative_path:
                    summary['files']['debug'].append(file_info)
                else:
                    summary['files']['other'].append(file_info)
        
        return summary
    
    def _get_folder_size(self, folder_path):
        """Calculate total size of folder in bytes"""
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(folder_path):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except:
                    pass
        return total_size
    
    def clean_old_extractions(self, days_old=7):
        """Clean up extraction folders older than specified days"""
        if not os.path.exists(self.base_directory):
            return []
        
        cutoff_date = datetime.now() - timedelta(days=days_old)
        cleaned_folders = []
        
        for item in os.listdir(self.base_directory):
            item_path = os.path.join(self.base_directory, item)
            if os.path.isdir(item_path):
                creation_time = datetime.fromtimestamp(os.path.getctime(item_path))
                if creation_time < cutoff_date:
                    try:
                        shutil.rmtree(item_path)
                        cleaned_folders.append(item_path)
                    except Exception as e:
                        print(f"Error cleaning {item_path}: {str(e)}")
        
        return cleaned_folders
    
def main():
    """CLI interface for file management utilities"""
    import argparse
    
    parser = argparse.ArgumentParser(description='File management utilities for job scraper')
    parser.add_argument('--clean', type=int, help='Clean extractions older than N days')
    parser.add_argument('--list', action='store_true', help='List recent extractions')
    parser.add_argument('--summary', type=str, help='Get summary of specific extraction folder')
    parser.add_argument('--export', nargs=2, help='Export data: <folder_path> <format>')
    
    args = parser.parse_args()
    
    fm = FileManager()
    
    if args.clean:
        cleaned = fm.cleanup_old_extractions(args.clean)
        print(f"Cleaned {cleaned} old extraction folders")
    
    elif args.list:
        extractions = fm.list_extractions()
        print(f"Recent extractions ({len(extractions)}):")
        for ext in extractions:
            print(f"  - {ext['id']} ({ext['size_mb']:.1f}MB, {ext['created_date']})")
    
    elif args.summary:
        summary = fm.get_extraction_summary(args.summary)
        if summary:
            print(f"Extraction Summary: {summary['id']}")
            print(f"Size: {summary['size_mb']:.1f}MB")
            print(f"Created: {summary['created_date']}")
            if 'files' in summary:
                print(f"Results files: {len(summary['files'].get('results', []))}")
                print(f"Debug files: {len(summary['files'].get('debug', []))}")
        else:
            print("Folder not found or invalid")
    
    elif args.export:
        folder_path, format_type = args.export
        try:
            exported_file = fm.export_results(folder_path, format_type)
            print(f"Data exported to: {exported_file}")
        except Exception as e:
            print(f"Export failed: {str(e)}")
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
